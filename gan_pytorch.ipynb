{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRPkNR05Zz60m4PwKOt9eA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frezaeix/easy-peasy/blob/master/gan_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul-YAcLA8qZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BQ0ZsGv8uZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "93586d46-44b7-4c2a-c6cf-0ec6421d7ed8"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Generative Adversarial Networks (GAN) example in PyTorch. Tested with PyTorch 0.4.1, Python 3.6.7 (Nov 2018)\n",
        "# See related blog post at https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f#.sch4xgsa9\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "matplotlib_is_available = True\n",
        "try:\n",
        "  from matplotlib import pyplot as plt\n",
        "except ImportError:\n",
        "  print(\"Will skip plotting; matplotlib is not available.\")\n",
        "  matplotlib_is_available = False\n",
        "\n",
        "# Data params\n",
        "data_mean = 4\n",
        "data_stddev = 1.25\n",
        "\n",
        "# ### Uncomment only one of these to define what data is actually sent to the Discriminator\n",
        "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
        "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
        "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
        "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n",
        "\n",
        "print(\"Using data [%s]\" % (name))\n",
        "\n",
        "# ##### DATA: Target data and generator input data\n",
        "\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
        "\n",
        "def get_generator_input_sampler():\n",
        "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
        "\n",
        "# ##### MODELS: Generator model and discriminator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map2(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map3(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(self.map1(x))\n",
        "        x = self.f(self.map2(x))\n",
        "        return self.f(self.map3(x))\n",
        "\n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "\n",
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]\n",
        "\n",
        "def get_moments(d):\n",
        "    # Return the first 4 moments of the data provided\n",
        "    mean = torch.mean(d)\n",
        "    diffs = d - mean\n",
        "    var = torch.mean(torch.pow(diffs, 2.0))\n",
        "    std = torch.pow(var, 0.5)\n",
        "    zscores = diffs / std\n",
        "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
        "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
        "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
        "    return final\n",
        "\n",
        "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
        "    mean = torch.mean(data.data, 1, keepdim=True)\n",
        "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
        "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
        "    if remove_raw_data:\n",
        "        return torch.cat([diffs], 1)\n",
        "    else:\n",
        "        return torch.cat([data, diffs], 1)\n",
        "\n",
        "def train():\n",
        "    # Model parameters\n",
        "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
        "    g_hidden_size = 5     # Generator complexity\n",
        "    g_output_size = 1     # Size of generated output vector\n",
        "    d_input_size = 500    # Minibatch size - cardinality of distributions\n",
        "    d_hidden_size = 10    # Discriminator complexity\n",
        "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
        "    minibatch_size = d_input_size\n",
        "\n",
        "    d_learning_rate = 1e-3\n",
        "    g_learning_rate = 1e-3\n",
        "    sgd_momentum = 0.9\n",
        "\n",
        "    num_epochs = 5000\n",
        "    print_interval = 100\n",
        "    d_steps = 20\n",
        "    g_steps = 20\n",
        "\n",
        "    dfe, dre, ge = 0, 0, 0\n",
        "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
        "\n",
        "    discriminator_activation_function = torch.sigmoid\n",
        "    generator_activation_function = torch.tanh\n",
        "\n",
        "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "    gi_sampler = get_generator_input_sampler()\n",
        "    G = Generator(input_size=g_input_size,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size=g_output_size,\n",
        "                  f=generator_activation_function)\n",
        "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
        "                      hidden_size=d_hidden_size,\n",
        "                      output_size=d_output_size,\n",
        "                      f=discriminator_activation_function)\n",
        "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
        "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for d_index in range(d_steps):\n",
        "            # 1. Train D on real+fake\n",
        "            D.zero_grad()\n",
        "\n",
        "            #  1A: Train D on real\n",
        "            d_real_data = Variable(d_sampler(d_input_size))\n",
        "            d_real_decision = D(preprocess(d_real_data))\n",
        "            d_real_error = criterion(d_real_decision, Variable(torch.ones([1,1])))  # ones = true\n",
        "            d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "            #  1B: Train D on fake\n",
        "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
        "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1,1])))  # zeros = fake\n",
        "            d_fake_error.backward()\n",
        "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
        "\n",
        "        for g_index in range(g_steps):\n",
        "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "            G.zero_grad()\n",
        "\n",
        "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "            g_fake_data = G(gen_input)\n",
        "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
        "            g_error = criterion(dg_fake_decision, Variable(torch.ones([1,1])))  # Train G to pretend it's genuine\n",
        "\n",
        "            g_error.backward()\n",
        "            g_optimizer.step()  # Only optimizes G's parameters\n",
        "            ge = extract(g_error)[0]\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
        "                  (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
        "\n",
        "    if matplotlib_is_available:\n",
        "        print(\"Plotting the generated distribution...\")\n",
        "        values = extract(g_fake_data)\n",
        "        print(\" Values: %s\" % (str(values)))\n",
        "        plt.hist(values, bins=50)\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Histogram of Generated Distribution')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using data [Only 4 moments]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: D (0.579020619392395 real_err, 0.8261594772338867 fake_err) G (0.5773526430130005 err); Real Dist ([3.976115233719349, 1.2655369606385363]),  Fake Dist ([-0.15788163439929484, 0.04176603199404299]) \n",
            "Epoch 100: D (0.6484960317611694 real_err, 0.5990088582038879 fake_err) G (0.7753793001174927 err); Real Dist ([3.91981565464288, 1.2140913535069686]),  Fake Dist ([4.579040807723999, 0.01143742559592755]) \n",
            "Epoch 200: D (0.6984226703643799 real_err, 0.6756894588470459 fake_err) G (0.6850152015686035 err); Real Dist ([4.084819043517113, 1.2847064970793924]),  Fake Dist ([6.217567507266998, 2.6167201793023382]) \n",
            "Epoch 300: D (0.7131206393241882 real_err, 0.6986618638038635 fake_err) G (0.6876441240310669 err); Real Dist ([3.952227130174637, 1.288541726719192]),  Fake Dist ([5.224152236938477, 1.850180722087432]) \n",
            "Epoch 400: D (0.47594037652015686 real_err, 0.36897513270378113 fake_err) G (0.4857286512851715 err); Real Dist ([3.908115881562233, 1.2254944164737607]),  Fake Dist ([8.999436277389526, 0.08058950866475303]) \n",
            "Epoch 500: D (0.9518201351165771 real_err, 0.7659668922424316 fake_err) G (0.28100502490997314 err); Real Dist ([3.954754596054554, 1.2368858711104684]),  Fake Dist ([4.753425455093383, 0.8785072460796891]) \n",
            "Epoch 600: D (0.41645994782447815 real_err, 0.5394148230552673 fake_err) G (0.7033299803733826 err); Real Dist ([4.0084334428310395, 1.2329172463095612]),  Fake Dist ([3.7608623037338256, 0.8383710095655225]) \n",
            "Epoch 700: D (0.9134896397590637 real_err, 0.8321290612220764 fake_err) G (0.5616092085838318 err); Real Dist ([3.9983919647186994, 1.2110417311923958]),  Fake Dist ([4.148149210453034, 0.9577787213406687]) \n",
            "Epoch 800: D (0.7120601534843445 real_err, 0.6877193450927734 fake_err) G (0.6995750069618225 err); Real Dist ([3.9900050158649685, 1.2251499534312515]),  Fake Dist ([4.298898196458817, 1.3437296410077562]) \n",
            "Epoch 900: D (0.6867309808731079 real_err, 0.6906946897506714 fake_err) G (0.692147970199585 err); Real Dist ([4.047375911492854, 1.2406366824857982]),  Fake Dist ([3.9479178862571715, 1.2397216341644282]) \n",
            "Epoch 1000: D (0.6887131333351135 real_err, 0.6963101625442505 fake_err) G (0.6959415674209595 err); Real Dist ([3.98890622164309, 1.2405596640600933]),  Fake Dist ([3.947057145833969, 1.238981434410926]) \n",
            "Epoch 1100: D (0.6889093518257141 real_err, 0.695870041847229 fake_err) G (0.6928052306175232 err); Real Dist ([3.9146476160138843, 1.2649292765057005]),  Fake Dist ([4.038781477451325, 1.230883287769411]) \n",
            "Epoch 1200: D (0.7121552228927612 real_err, 0.6950386762619019 fake_err) G (0.6889942288398743 err); Real Dist ([4.069722446918488, 1.2079960741385718]),  Fake Dist ([4.084725248575211, 1.246186441289878]) \n",
            "Epoch 1300: D (0.6909264922142029 real_err, 0.690043032169342 fake_err) G (0.6923616528511047 err); Real Dist ([4.024261536089703, 1.2611451319137426]),  Fake Dist ([3.886989699602127, 1.2925056877024956]) \n",
            "Epoch 1400: D (0.6880999803543091 real_err, 0.6879958510398865 fake_err) G (0.6942118406295776 err); Real Dist ([3.998961538016796, 1.219936917373175]),  Fake Dist ([4.130591956615448, 1.255760865141169]) \n",
            "Epoch 1500: D (0.6995497345924377 real_err, 0.6944499015808105 fake_err) G (0.7048866748809814 err); Real Dist ([4.034023659467697, 1.2309044312891626]),  Fake Dist ([4.047951163053512, 1.2661988858948774]) \n",
            "Epoch 1600: D (0.6888518929481506 real_err, 0.6986190676689148 fake_err) G (0.6901427507400513 err); Real Dist ([4.02815050059557, 1.267884823413214]),  Fake Dist ([3.9568306677341463, 1.1942735830508193]) \n",
            "Epoch 1700: D (0.6907885670661926 real_err, 0.6949955821037292 fake_err) G (0.6899988055229187 err); Real Dist ([3.981414732626872, 1.3052573431396226]),  Fake Dist ([4.0422801814079286, 1.2140194022845754]) \n",
            "Epoch 1800: D (0.6977026462554932 real_err, 0.6867629885673523 fake_err) G (0.6932127475738525 err); Real Dist ([4.05717203938961, 1.238034448740278]),  Fake Dist ([4.206861897706985, 1.3428973163915545]) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZlHJW1v8z_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}